{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae5523a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import  numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2591400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "credit = pd.read_csv('credit_fraud/creditcard.csv')\n",
    "\n",
    "#Normalize amount and time as the other datapoints are normalized via PCA\n",
    "scaler = StandardScaler()\n",
    "\n",
    "credit[['Amount', 'Time']] = scaler.fit_transform(credit[['Amount', 'Time']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f6ebddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the labels and feature splits\n",
    "\n",
    "#drop the classes from the features it should be the labels\n",
    "X = credit.drop('Class', axis = 1)\n",
    "labels = credit['Class']\n",
    "\n",
    "#split the data\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(X, labels, test_size=0.2, random_state=42, stratify = labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b8e09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply an oversampler to cover for the large class disparity\n",
    "oversampler = RandomOverSampler(random_state = 42)\n",
    "features_train, labels_train = oversampler.fit_resample(features_train, labels_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99d853ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the classifiers\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter = 1000, random_state = 42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder = False, eval_metric = 'logloss', random_state = 42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state = 42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c19796",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [56962, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m labels_prob = model.predict_proba(features_test)[:,\u001b[32m1\u001b[39m]\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m#get the metrics\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m report = \u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m roc_auc = roc_auc_score(labels_test, labels_prob)\n\u001b[32m     13\u001b[39m precision, recall, _ = precision_recall_curve(labels_test, labels_prob)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jomar\\VSCode\\ML\\BasicML\\ML\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jomar\\VSCode\\ML\\BasicML\\ML\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2671\u001b[39m, in \u001b[36mclassification_report\u001b[39m\u001b[34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[39m\n\u001b[32m   2563\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Build a text report showing the main classification metrics.\u001b[39;00m\n\u001b[32m   2564\u001b[39m \n\u001b[32m   2565\u001b[39m \u001b[33;03mRead more in the :ref:`User Guide <classification_report>`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2667\u001b[39m \u001b[33;03m<BLANKLINE>\u001b[39;00m\n\u001b[32m   2668\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2670\u001b[39m y_true, y_pred = attach_unique(y_true, y_pred)\n\u001b[32m-> \u001b[39m\u001b[32m2671\u001b[39m y_type, y_true, y_pred = \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2674\u001b[39m     labels = unique_labels(y_true, y_pred)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jomar\\VSCode\\ML\\BasicML\\ML\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:98\u001b[39m, in \u001b[36m_check_targets\u001b[39m\u001b[34m(y_true, y_pred)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[32m     72\u001b[39m \n\u001b[32m     73\u001b[39m \u001b[33;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     95\u001b[39m \u001b[33;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     97\u001b[39m xp, _ = get_namespace(y_true, y_pred)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m type_true = type_of_target(y_true, input_name=\u001b[33m\"\u001b[39m\u001b[33my_true\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    100\u001b[39m type_pred = type_of_target(y_pred, input_name=\u001b[33m\"\u001b[39m\u001b[33my_pred\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jomar\\VSCode\\ML\\BasicML\\ML\\Lib\\site-packages\\sklearn\\utils\\validation.py:475\u001b[39m, in \u001b[36mcheck_consistent_length\u001b[39m\u001b[34m(*arrays)\u001b[39m\n\u001b[32m    473\u001b[39m uniques = np.unique(lengths)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    476\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    477\u001b[39m         % [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[32m    478\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Found input variables with inconsistent numbers of samples: [56962, 1]"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "#dict to store results per model\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    #fit the model\n",
    "    model.fit(features_train, labels_train)\n",
    "    labels_pred = model.predict(features_test)\n",
    "    labels_prob = model.predict_proba(features_test)[:, 1]\n",
    "\n",
    "\n",
    "    #get the metrics\n",
    "    report = classification_report(labels_test, labels_pred, output_dict = True)\n",
    "    roc_auc = roc_auc_score(labels_test, labels_prob)\n",
    "    precision, recall, _ = precision_recall_curve(labels_test, labels_prob)\n",
    "    pr_auc = auc(recall, precision)\n",
    "\n",
    "    results[model_name] = {\n",
    "        \"Classification Report\": report,\n",
    "        \"ROC AUC\": roc_auc,\n",
    "        \"PR AUC\": pr_auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b958c863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
