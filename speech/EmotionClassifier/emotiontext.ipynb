{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "586e049c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jomar\\VSCode\\ML\\BasicML\\ML\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizerFast, DistilBertModel\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22f6fec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05663024",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "#data came from GoEmotions Dataset\n",
    "#tsvs are tab separated vals, so specify the separator moreover it has no headers/col names\n",
    "data = pd.read_csv(\"train.tsv\", sep = \"\\t\", header = None, names = [\"text\",\"labels\", \"id\"])\n",
    "\n",
    "#convert lists of strings into comma separated vals (some text have multiple emotions separated by comma)\n",
    "data[\"labels\"] = data[\"labels\"].apply(lambda x: list(map(int, x.split(\",\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a560346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "#load up emotion labels\n",
    "with open(\"emotions.txt\", \"r\") as f:\n",
    "    LABELS = list((line.strip() for line in f.readlines()))\n",
    "NUM_CLASSES = len(LABELS)\n",
    "print(LABELS)\n",
    "print(NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8212e55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#binarize labels to indicate precense of certain emtion 1 for yes 0 for no\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes = range(NUM_CLASSES))\n",
    "y = mlb.fit_transform(data[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b14e66ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing phase, turn text into digits\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8634c12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a custom dataset\n",
    "class GoEmotionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenzer, max_len = 64):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return item\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d2fd6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare dataset\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    data[\"text\"], y, test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d10a30fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GoEmotionDataset(train_texts.tolist(), train_labels, tokenizer)\n",
    "val_dataset = GoEmotionDataset(val_texts.tolist(), val_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77afd917",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataloaders\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers= 4, persistent_workers= True, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52132bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the model\n",
    "\n",
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, NUM_CLASSES)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        return self.fc(cls_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8a8f8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jomar\\AppData\\Local\\Temp\\ipykernel_26952\\323087356.py:8: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "#training setup\n",
    "\n",
    "model = EmotionClassifier().to(device)\n",
    "#adamW is standard for huggingface transformers, adamW introduces weight decays which prevent overfitting\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr =  0.00005, weight_decay= 0.01)\n",
    "#standard loss for transformers\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b55f4b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, device, num_epochs=3, threshold=0.5):\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            total_samples = 0\n",
    "\n",
    "            for batch in dataloaders[phase]:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    with autocast():\n",
    "                        outputs = model(input_ids, attention_mask)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        scaler.scale(loss).backward()\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "\n",
    "                # Update loss\n",
    "                batch_size = input_ids.size(0)\n",
    "                running_loss += loss.item() * batch_size\n",
    "                total_samples += batch_size\n",
    "\n",
    "                # Compute accuracy (multi-label: threshold sigmoid outputs)\n",
    "                preds = (torch.sigmoid(outputs) > threshold).float()\n",
    "                corrects = (preds == labels).float().sum()\n",
    "                running_corrects += corrects\n",
    "\n",
    "            epoch_loss = running_loss / total_samples\n",
    "            epoch_acc = running_corrects / (total_samples * labels.size(1))\n",
    "\n",
    "            print(f\"{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17514ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, val_loader, device, threshold = 0.5):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].cpu().numpy()\n",
    "            outputs = torch.sigmoid(model(input_ids, attention_mask)).cpu().numpy()\n",
    "            preds = (outputs > threshold).astype(int)\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    report = classification_report(all_labels, all_preds, target_names=LABELS, zero_division=0)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078a4a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jomar\\AppData\\Local\\Temp\\ipykernel_26952\\3777761098.py:2: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n"
     ]
    }
   ],
   "source": [
    "dataloaders = {'train': train_loader, 'val': val_loader}\n",
    "train_model(model, dataloaders, criterion, optimizer, device)\n",
    "eval_model(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4b37ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model for reuse\n",
    "\n",
    "os.makedirs(\"saved_model\", exist_ok = True)\n",
    "torch.save(model.state_dict(), \"saved_model/emotion.pt\")\n",
    "tokenizer.save_pretrained(\"saved_model\")\n",
    "with open(\"saved_model/label_list.json\", \"w\") as f:\n",
    "    json.dump(LABELS, f)\n",
    "\n",
    "print(\"âœ… Model and files saved to 'saved_model/'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
